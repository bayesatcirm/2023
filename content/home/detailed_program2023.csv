Day,Hour,talk_type,Speaker,Title,Abstract
Monday,8:45-9:00,Introduction,Organizers,"Opening remarks",
Monday,9:00-11:00,Master Class,Nicolas Chopin,"An introduction to state-space models, particle filters, and Sequential Monte Carlo samplers"," This course will provide a general introduction to SMC algorithms, from basic particle filters and their uses in state-space (hidden Markov) modelling in various areas, to more advanced algorithms such as SMC samplers, which may be used to sample from one, or several target distributions. The course will cover ""a bit of everything"": theory (using Feynman-Kac models as a general framework), methodology (how to construct better algorithms in practice), implementation (examples in Python based on the library particles will be showcased), and applications."
Monday,11:00-11:30,Coffee Break,,,
Monday,11:30-12:30,Invited talk,François Caron,,
Monday,12:30-14:00,Lunch,,,
Monday,14:00-16:00,Tutorial,Gabriel Victorino Cardoso and Yazid Janati El Idrissi,,
Monday,16:00-16:30,Coffee break,,,
Monday,16:30-17:00,Contributed,Liam Llamazares,"Penalized complexity priors for stochastic partial differential equations","Penalized complexity priors are used in Bayesian inference to define priors that penalize the distance from a base model. We investigate how to use this framework to set priors for the coefficients of a spatial stochastic partial differential equation (SPDE).   We first work with stationary solutions to the SPDE. In this case, the coefficients are finite-dimensional and not functions of space, leading to a more straightforward analysis.   We then extend this to the non-stationary setting. In this case, as the coefficients are themselves functions of space, the parameter space is infinite-dimensional. We show how to extend the concept of spectral density to non-stationary fields. We then use this non-stationary spectral density to define a distance between the parameters of the SPDE and calculate a penalized complexity prior."
Monday,17:00-17:30,Contributed,Jacopo Iollo,"Tempered sequential Monte Carlo for Bayesian experimental design via stochastic optimization","We propose a new procedure, for Bayesian experimental design, that performs sequential design optimization while simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and tempered Sequential Monte Carlo (SMC) samplers to maximise the  Expected Information Gain (EIG).  As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective worsens classical SMC performance.  To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling.  %tempering is proposed to improve SMC sampling accuracy.  This novel combination of stochastic optimization and  tempered SMC  allows to jointly handle design optimization and  parameter posterior inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the approach potential on various benchmarks where our procedure outperforms other recent existing approaches."
Monday,17:30-19:30,Poster session,,,
Monday,19:30-21:00,Dinner,,,
Tuesday,9:00-11:00,Master Class,Nicolas Chopin,"An introduction to state-space models, particle filters, and Sequential Monte Carlo samplers",
Tuesday,11:00-11:30,Coffee Break,,,
Tuesday,11:30-12:00,Contributed,David Agnoletto,"Bayesian inference for generalized linear models via quasi-posteriors","Generalized linear models are a standard statistical tool for modeling the relation between a response variable and a set of covariates. Despite their popularity, they can incur in misspecification problems that could negatively impact inferential conclusions. A semi-parametric solution adopted in frequentist literature uses the quasi-likelihood function, which relies on the second-order assumptions, in place of the usual likelihood. This approach yields increased flexibility since only the first two moments of the data generator are specified rather than its entire distribution. We propose to integrate this solution in the Bayesian paradigm introducing the quasi-posterior distribution. This quantity represents a coherent Bayesian update according to the generalized Bayes notion. We show that quasi-posterior approximates the regression coarsened posterior in the case of exponential families, providing new insights on the choice of the coarsening parameter. Asymptotically, the quasi-posterior converges in total variation to a normal distribution, has important connections with the loss-likelihood bootstrap posterior, and is also well-calibrated in terms of frequentist coverage. Moreover, the loss-scale parameter has a clear interpretation in terms of the dispersion parameter, leading to the consolidated method of moments estimator for its quantification. We provide some applications to overdispersed counts and heteroscedastic continuous data."
Tuesday,12:00-12:30,Contributed,Sylvain Le Corff,"Monte Carlo guided Diffusion for Bayesian linear inverse problems","Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems."
Tuesday,12:30-14:00,Lunch,,,
Tuesday,14:00-15:00,Invited talk,Marta Catalano,Merging rate of opinions via optimal transport on random measures,"The Bayesian approach to inference is based on a coherent probabilistic framework that naturally leads to principled uncertainty quantification and prediction. Via posterior distributions, Bayesian nonparametric models make inference on parameters belonging to infinite-dimensional spaces, such as the space of probability distributions. The development of Bayesian nonparametrics has been triggered by the Dirichlet process, a nonparametric prior that allows one to learn the law of the observations through closed-form expressions. Still, its learning mechanism is often too simplistic and many generalizations have been proposed to increase its flexibility, a popular one being the class of normalized completely random measures. Here we investigate a simple yet fundamental matter: will a different prior actually guarantee a different learning outcome? To this end, we develop a new distance between completely random measures based on optimal transport, which provides an original framework for quantifying the similarity between posterior distributions (merging of opinions). Our findings provide neat and interpretable insights on the impact of popular Bayesian nonparametric priors, avoiding the usual restrictive assumptions on the data-generating process. This is joint work with Hugo Lavenant."
Tuesday,15:00-15:30,Contributed,Francesca Crucinio,"Optimal Scaling Results for a Wide Class of Proximal MALA Algorithms","We consider a recently proposed class of MCMC methods which uses proximity maps instead of gradients to build proposal mechanisms which can be employed for both differentiable and non-differentiable targets. These methods have been shown to be stable for a wide class of targets, making them a valuable alternative to Metropolis-adjusted Langevin algorithms (MALA); and have found wide application in imaging contexts. The wider stability properties are obtained by building the Moreau-Yoshida envelope for the target of interest, which depends on a parameter λ. In this work, we investigate the optimal scaling problem for this class of algorithms, which encompasses MALA, and provide practical guidelines for the implementation of these methods."
Tuesday,15:30-16:00,Contributed,Mikolaj Kasprzak,"How good is your Laplace approximation of the Bayesian posterior?", "The Laplace approximation is a popular method for providing posterior mean and variance estimates. But can we trust these estimates for practical use? One might consider using rate-of-convergence bounds for the Bayesian Central Limit Theorem (BCLT) to provide quality guarantees for the Laplace approximation. But the bounds in existing versions of the BCLT either: require knowing the true data-generating parameter, are asymptotic in the number of samples, do not control the Bayesian posterior mean, or apply only to narrow classes of models. Our work provides the first closed-form, finite-sample quality bounds for the Laplace approximation that simultaneously (1) do not require knowing the true parameter, (2) control posterior means and variances, and (3) apply generally to models that satisfy the conditions of the asymptotic BCLT. In fact, our bounds work even in the presence of misspecification. We compute exact constants in our bounds for a variety of standard models, including logistic regression, and numerically demonstrate their utility. We provide a framework for analysis of more complex models."
Tuesday,16:00-16:30,Coffee break,,,
Tuesday,16:30-17:30,Invited talk,Jean-Michel Marin,,
Tuesday,18:00-19:00,Social event,Football game,,
Tuesday,19:30-21:00,Dinner,,,
Wednesday,9:00-11:00,Master Class,Silvia Chiappa,,
Wednesday,11:00-11:30,Coffee break,,,
Wednesday,11:30-12:00,Contributed,,,
Wednesday,12:00-12:30,Contributed,Mengyan Zhang,,
Wednesday,12:30-14:00,Lunch,,,
Wednesday,14:00-19:30,Social event,Free time,,
Wednesday,19:30-21:00,Dinner,,,
Thursday,9:00-11:00,Master Class,Silvia Chiappa,,
Thursday,11:00-11:30,Coffee break,,,
Thursday,11:30-12:00,Contributed,Elena Bortolato,,
Thursday,12:00-12:30,Contributed,Thibaut Lemoine,,
Thursday,12:30-14:00,Lunch,,,
Thursday,14:00-16:00,Tutorial,Gabriel Victorino Cardoso and Yazid Janati El Idrissi,,
Thursday,16:00-16:30,Coffee break,,,
Thursday,16:30-17:30,Invited talk,Fabrizia Mealli,,
Thursday,17:30-18:00,Snooze time,,,
Thursday,18:00-19:30,Social event,,,
Thursday,19:30-21:00,Dinner,,,
Friday,9:30-10:00,Contributed,Claudio del Sole,,
Friday,10:00-10:30,Contributed,,,
Friday,10:30-11:00,Contributed,Filippo Ascolani,,
Friday,11:00-11:30,Coffee break,,,
Friday,11:30-12:30,Invited talk,Sophie Donnet,Using a Sequential Monte Carlo algorithm to find the mesoscale structure of a network ,"This work is motivated by the analysis of ecological interaction networks. Stochastic block models are widely used in this field to decipher the structure that underlies a network or that is shared by a collection of networks. Efficient algorithms based on variational approximations exist for frequentist inference and sometimes for Bayesian inference, but without statistical guaranties as for the resulting estimates. We propose to combine the variational estimation with a sequential Monte-Carlo algorithm to efficiently sample the posterior distribution and to perform model selection. "
Friday,12:30-14:00,Lunch,,,
