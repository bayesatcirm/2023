Day,Hour,talk_type,Speaker,Title,Abstract
Monday,8:45-9:00,Introduction,Organizers,"Opening remarks",
Monday,9:00-11:00,Master Class,Nicolas Chopin,"An introduction to state-space models, particle filters, and Sequential Monte Carlo samplers"," This course will provide a general introduction to SMC algorithms, from basic particle filters and their uses in state-space (hidden Markov) modelling in various areas, to more advanced algorithms such as SMC samplers, which may be used to sample from one, or several target distributions. The course will cover ""a bit of everything"": theory (using Feynman-Kac models as a general framework), methodology (how to construct better algorithms in practice), implementation (examples in Python based on the library particles will be showcased), and applications."
Monday,11:00-11:30,Coffee Break,,,
Monday,11:30-12:30,Invited talk,François Caron,,
Monday,12:30-14:00,Lunch,,,
Monday,14:00-16:00,Tutorial,Gabriel Victorino Cardoso and Yazid Janati El Idrissi,,
Monday,16:00-16:30,Coffee break,,,
Monday,16:30-17:00,Contributed,Liam Llamazares,"Penalized complexity priors for stochastic partial differential equations","Penalized complexity priors are used in Bayesian inference to define priors that penalize the distance from a base model. We investigate how to use this framework to set priors for the coefficients of a spatial stochastic partial differential equation (SPDE).   We first work with stationary solutions to the SPDE. In this case, the coefficients are finite-dimensional and not functions of space, leading to a more straightforward analysis.   We then extend this to the non-stationary setting. In this case, as the coefficients are themselves functions of space, the parameter space is infinite-dimensional. We show how to extend the concept of spectral density to non-stationary fields. We then use this non-stationary spectral density to define a distance between the parameters of the SPDE and calculate a penalized complexity prior."
Monday,17:00-17:30,Contributed,Jacopo Iollo,"Tempered sequential Monte Carlo for Bayesian experimental design via stochastic optimization","We propose a new procedure, for Bayesian experimental design, that performs sequential design optimization while simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and tempered Sequential Monte Carlo (SMC) samplers to maximise the  Expected Information Gain (EIG).  As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective worsens classical SMC performance.  To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling.  %tempering is proposed to improve SMC sampling accuracy.  This novel combination of stochastic optimization and  tempered SMC  allows to jointly handle design optimization and  parameter posterior inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the approach potential on various benchmarks where our procedure outperforms other recent existing approaches."
Monday,17:30-19:30,Poster session,,,
Monday,19:30-21:00,Dinner,,,
Tuesday,9:00-11:00,Master Class,Nicolas Chopin,"An introduction to state-space models, particle filters, and Sequential Monte Carlo samplers",
Tuesday,11:00-11:30,Coffee Break,,,
Tuesday,11:30-12:00,Contributed,David Agnoletto,"Bayesian inference for generalized linear models via quasi-posteriors","Generalized linear models are a standard statistical tool for modeling the relation between a response variable and a set of covariates. Despite their popularity, they can incur in misspecification problems that could negatively impact inferential conclusions. A semi-parametric solution adopted in frequentist literature uses the quasi-likelihood function, which relies on the second-order assumptions, in place of the usual likelihood. This approach yields increased flexibility since only the first two moments of the data generator are specified rather than its entire distribution. We propose to integrate this solution in the Bayesian paradigm introducing the quasi-posterior distribution. This quantity represents a coherent Bayesian update according to the generalized Bayes notion. We show that quasi-posterior approximates the regression coarsened posterior in the case of exponential families, providing new insights on the choice of the coarsening parameter. Asymptotically, the quasi-posterior converges in total variation to a normal distribution, has important connections with the loss-likelihood bootstrap posterior, and is also well-calibrated in terms of frequentist coverage. Moreover, the loss-scale parameter has a clear interpretation in terms of the dispersion parameter, leading to the consolidated method of moments estimator for its quantification. We provide some applications to overdispersed counts and heteroscedastic continuous data."
Tuesday,12:00-12:30,Contributed,Sylvain Le Corff,"Monte Carlo guided Diffusion for Bayesian linear inverse problems","Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems."
Tuesday,12:30-14:00,Lunch,,,
Tuesday,14:00-15:00,Invited talk,Marta Catalano,Merging rate of opinions via optimal transport on random measures,"The Bayesian approach to inference is based on a coherent probabilistic framework that naturally leads to principled uncertainty quantification and prediction. Via posterior distributions, Bayesian nonparametric models make inference on parameters belonging to infinite-dimensional spaces, such as the space of probability distributions. The development of Bayesian nonparametrics has been triggered by the Dirichlet process, a nonparametric prior that allows one to learn the law of the observations through closed-form expressions. Still, its learning mechanism is often too simplistic and many generalizations have been proposed to increase its flexibility, a popular one being the class of normalized completely random measures. Here we investigate a simple yet fundamental matter: will a different prior actually guarantee a different learning outcome? To this end, we develop a new distance between completely random measures based on optimal transport, which provides an original framework for quantifying the similarity between posterior distributions (merging of opinions). Our findings provide neat and interpretable insights on the impact of popular Bayesian nonparametric priors, avoiding the usual restrictive assumptions on the data-generating process. This is joint work with Hugo Lavenant."
Tuesday,15:00-15:30,Contributed,Francesca Crucinio,"Optimal Scaling Results for a Wide Class of Proximal MALA Algorithms","We consider a recently proposed class of MCMC methods which uses proximity maps instead of gradients to build proposal mechanisms which can be employed for both differentiable and non-differentiable targets. These methods have been shown to be stable for a wide class of targets, making them a valuable alternative to Metropolis-adjusted Langevin algorithms (MALA); and have found wide application in imaging contexts. The wider stability properties are obtained by building the Moreau-Yoshida envelope for the target of interest, which depends on a parameter λ. In this work, we investigate the optimal scaling problem for this class of algorithms, which encompasses MALA, and provide practical guidelines for the implementation of these methods."
Tuesday,15:30-16:00,Contributed,Mikolaj Kasprzak,"How good is your Laplace approximation of the Bayesian posterior?", "The Laplace approximation is a popular method for providing posterior mean and variance estimates. But can we trust these estimates for practical use? One might consider using rate-of-convergence bounds for the Bayesian Central Limit Theorem (BCLT) to provide quality guarantees for the Laplace approximation. But the bounds in existing versions of the BCLT either: require knowing the true data-generating parameter, are asymptotic in the number of samples, do not control the Bayesian posterior mean, or apply only to narrow classes of models. Our work provides the first closed-form, finite-sample quality bounds for the Laplace approximation that simultaneously (1) do not require knowing the true parameter, (2) control posterior means and variances, and (3) apply generally to models that satisfy the conditions of the asymptotic BCLT. In fact, our bounds work even in the presence of misspecification. We compute exact constants in our bounds for a variety of standard models, including logistic regression, and numerically demonstrate their utility. We provide a framework for analysis of more complex models."
Tuesday,16:00-16:30,Coffee break,,,
Tuesday,16:30-17:30,Invited talk,Jean-Michel Marin,,
Tuesday,18:00-19:00,Social event,Football game,,
Tuesday,19:30-21:00,Dinner,,,
Wednesday,9:00-11:00,Master Class,Silvia Chiappa,,
Wednesday,11:00-11:30,Coffee break,,,
Wednesday,11:30-12:00,Contributed,Joshua Bon,"Bayesian score calibration for approximate models","We propose a new method for adjusting samples from an approximate posterior to reduce bias and produce more accurate uncertainty quantification. We do this by optimising a transform of the approximate posterior that maximises a scoring rule. The procedure is Bayesian but has some interesting parallels with frequentist calibration."
Wednesday,12:00-12:30,Contributed,Mengyan Zhang,"Bayesian optimisation with aggregated feedback","We consider the Bayesian optimisation problem, under a novel setting of aggregated feedback. This is motivated by applications where the precise rewards are impossible or expensive to obtain, while an aggregated reward or feedback, such as the average over a subset, is available. We adaptively construct a tree with nodes as subsets of the arm space and propose Gaussian Process Optimistic Optimisation (GPOO) algorithm."
Wednesday,12:30-14:00,Lunch,,,
Wednesday,14:00-19:30,Social event,Free time,,
Wednesday,19:30-21:00,Dinner,,,
Thursday,9:00-11:00,Master Class,Silvia Chiappa,,
Thursday,11:00-11:30,Coffee break,,,
Thursday,11:30-12:00,Contributed,Elena Bortolato,"Coupling MCMC algorithms on submanifolds","Manifolds arise in diverse problems in statistics, demanding effective methods for sampling probability distributions defined on them and specialized Markov Chain Monte Carlo (MCMC) methods since Andersen (1983) were developed.Such techniques can be also applied to a variety of sampling problems, even not  naturally defined on manifolds. We propose to combine standard MCMC samplers with such manifold steps and develop implementable coupling schemes to diagnose convergence and to evaluate the efficiency."
Thursday,12:00-12:30,Contributed,Thibaut Lemoine,"Monte Carlo integration on complex manifolds","TBC"
Thursday,12:30-14:00,Lunch,,,
Thursday,14:00-16:00,Tutorial,Gabriel Victorino Cardoso and Yazid Janati El Idrissi,,
Thursday,16:00-16:30,Coffee break,,,
Thursday,16:30-17:30,Invited talk,Fabrizia Mealli,,
Thursday,17:30-18:00,Contributed,Filippo Ascolani,"Complexity of Gibbs samplers through Bayesian asymptotics","Gibbs samplers are popular algorithms to approximate posterior distributions arising from Bayesian hierarchical models.  Despite their popularity and good empirical performances, however, there are still relatively few quantitative theoretical results on their scalability or lack thereof, e.g. much less than for gradient-based sampling methods. We introduce a novel technique to analyse the asymptotic behaviour of mixing times of Gibbs Samplers, based on tools of Bayesian asymptotics.  We apply our methodology to high-dimensional hierarchical models, obtaining dimension-free convergence results for Gibbs samplers under random data-generating assumptions, for a broad class of two-level models with generic likelihood function. Specific examples with Gaussian, binomial and categorical likelihoods are discussed."
Thursday,18:00-19:30,Social event,,,
Thursday,19:30-21:00,Dinner,,,
Friday,9:00-10:00,Invited talk,Sophie Donnet,"Using a Sequential Monte Carlo algorithm to find the mesoscale structure of a network","This work is motivated by the analysis of ecological interaction networks. Stochastic block models are widely used in this field to decipher the structure that underlies a network or that is shared by a collection of networks. Efficient algorithms based on variational approximations exist for frequentist inference and sometimes for Bayesian inference, but without statistical guaranties as for the resulting estimates. We propose to combine the variational estimation with a sequential Monte-Carlo algorithm to efficiently sample the posterior distribution and to perform model selection. "
Friday,10:00-10:30,Contributed,Anna Menacher,"Scalar-on-image regression with a relaxed Gaussian process prior","In this work, we provide a scalable hierarchical Bayesian spatial model for scalar-on-image regression problems with a relaxed thresholded Gaussian process prior on the spatially-varying parameters. We achieve the same properties as the soft thresholded Gaussian process prior, developed by Kang et al. (2018), by introducing an additional set of parameters that perform the thresholding; however, our algorithm does not rely on a Metropolis-within-Gibbs sampler to sample the parameters. We aim to utilize variational inference and thereby speed up the performance of the algorithm to enable its application to large-scale population health studies as well as to increase the number of imaging modalities that can be introduced to the model. We support our work with extensive simulation studies and provide a real data application to the UK Biobank."
Friday,10:30-11:00,Contributed,Claudio del Sole,"Hierarchically dependent mixture hazard rates for modelling competing risks","A popular approach in Bayesian modelling of partially exchangeable data consists in imposing hierarchical nonparametric priors, which induce dependence across groups of observations. In survival analysis, hierarchies of completely random measures have been successfully exploited as mixing measures to model multivariate dependent mixture hazard rates, leading to a posterior characterization which may also accommodate censored observations. Such framework can be easily adapted to a competing risks scenario, in which groups correspond to different diseases affecting each individual: in this case, the multivariate construction acts at a latent level, as only the minimum time-to-event and the corresponding cause of death are actually observed. The posterior hierarchy of random measures, as well as the posterior estimates of both survival function and cause-specific incidence functions are explicitly described, conditionally on a suitable latent partition structure which fits the Chinese restaurant franchise metaphor. Marginal and conditional sampling algorithms are also devised and tested on synthetic datasets. The performances of this proposal are finally compared with those of its non-hierarchical counterpart, which models the hazard rate of each disease independently: leveraging the information borrowed from other groups, the hierarchical construction is empirically shown to recover the shape of the incidence functions more efficiently, in presence of proportional hazards."
Friday,11:00-11:30,Coffee break,,,
Friday,11:30-12:00,Contributed,Felipe Uribe,"Towards dimension reduction of Bayesian inverse problems with neural network priors","In many Bayesian inverse problems the change from prior to posterior is confined to a low-dimensional subspace of the parameter space. We explore gradient-based dimension reduction techniques to identify this crucial subspace, with the primary aim of enhancing the efficiency of MCMC methods employed in tackling the inverse problem."
Friday,12:00-12:30,Introduction,Organizers,"Closing remarks","Bye everyone! Hope you enjoyed the conference!"
Friday,12:30-14:00,Lunch,,,
