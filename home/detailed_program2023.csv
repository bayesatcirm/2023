Day,Hour,talk_type,Speaker,Title,Abstract
Monday,8:45-9:00,Introduction,Organizers,Opening remarks,
Monday,9:00-11:00,Master Class,Nicolas Chopin,"An introduction to state-space models, particle filters, and Sequential Monte Carlo samplers"," This course will provide a general introduction to SMC algorithms, from basic particle filters and their uses in state-space (hidden Markov) modelling in various areas, to more advanced algorithms such as SMC samplers, which may be used to sample from one, or several target distributions. The course will cover ""a bit of everything"": theory (using Feynman-Kac models as a general framework), methodology (how to construct better algorithms in practice), implementation (examples in Python based on the library particles will be showcased), and applications."
Monday,11:00-11:30,Social event, Coffee Break,,
Monday,11:30-12:30,Invited talk,Marta Catalano,Merging rate of opinions via optimal transport on random measures,"The Bayesian approach to inference is based on a coherent probabilistic framework that naturally leads to principled uncertainty quantification and prediction. Via posterior distributions, Bayesian nonparametric models make inference on parameters belonging to infinite-dimensional spaces, such as the space of probability distributions. The development of Bayesian nonparametrics has been triggered by the Dirichlet process, a nonparametric prior that allows one to learn the law of the observations through closed-form expressions. Still, its learning mechanism is often too simplistic and many generalizations have been proposed to increase its flexibility, a popular one being the class of normalized completely random measures. Here we investigate a simple yet fundamental matter: will a different prior actually guarantee a different learning outcome? To this end, we develop a new distance between completely random measures based on optimal transport, which provides an original framework for quantifying the similarity between posterior distributions (merging of opinions). Our findings provide neat and interpretable insights on the impact of popular Bayesian nonparametric priors, avoiding the usual restrictive assumptions on the data-generating process. This is joint work with Hugo Lavenant."
Monday,12:30-14:00,Social event, Lunch,,
Monday,14:00-16:00,Tutorial,Gabriel Victorino Cardoso and Yazid Janati El Idrissi,,
Monday,16:00-16:30,Social event, Coffee Break,,
Monday,16:30-17:00,Contributed,Liam Llamazares,Penalized complexity priors for stochastic partial differential equations,"Penalized complexity priors are used in Bayesian inference to define priors that penalize the distance from a base model. We investigate how to use this framework to set priors for the coefficients of a spatial stochastic partial differential equation (SPDE).   We first work with stationary solutions to the SPDE. In this case, the coefficients are finite-dimensional and not functions of space, leading to a more straightforward analysis.   We then extend this to the non-stationary setting. In this case, as the coefficients are themselves functions of space, the parameter space is infinite-dimensional. We show how to extend the concept of spectral density to non-stationary fields. We then use this non-stationary spectral density to define a distance between the parameters of the SPDE and calculate a penalized complexity prior."
Monday,17:00-17:30,Contributed,Joshua Bon,Bayesian score calibration for approximate models,We propose a new method for adjusting samples from an approximate posterior to reduce bias and produce more accurate uncertainty quantification. We do this by optimising a transform of the approximate posterior that maximises a scoring rule. The procedure is Bayesian but has some interesting parallels with frequentist calibration.
Monday,17:30-19:30,Poster session,,,
Monday,19:30-21:00,Social event, Dinner,,
Tuesday,9:00-11:00,Master Class,Nicolas Chopin,"An introduction to state-space models, particle filters, and Sequential Monte Carlo samplers",
Tuesday,11:00-11:30,Social event, Coffee Break,,
Tuesday,11:30-12:00,Contributed,David Agnoletto,Bayesian inference for generalized linear models via quasi-posteriors,"Generalized linear models are a standard statistical tool for modeling the relation between a response variable and a set of covariates. Despite their popularity, they can incur in misspecification problems that could negatively impact inferential conclusions. A semi-parametric solution adopted in frequentist literature uses the quasi-likelihood function, which relies on the second-order assumptions, in place of the usual likelihood. This approach yields increased flexibility since only the first two moments of the data generator are specified rather than its entire distribution. We propose to integrate this solution in the Bayesian paradigm introducing the quasi-posterior distribution. This quantity represents a coherent Bayesian update according to the generalized Bayes notion. We show that quasi-posterior approximates the regression coarsened posterior in the case of exponential families, providing new insights on the choice of the coarsening parameter. Asymptotically, the quasi-posterior converges in total variation to a normal distribution, has important connections with the loss-likelihood bootstrap posterior, and is also well-calibrated in terms of frequentist coverage. Moreover, the loss-scale parameter has a clear interpretation in terms of the dispersion parameter, leading to the consolidated method of moments estimator for its quantification. We provide some applications to overdispersed counts and heteroscedastic continuous data."
Tuesday,12:00-12:30,Contributed,Sylvain Le Corff,Monte Carlo guided Diffusion for Bayesian linear inverse problems,"Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems."
Tuesday,12:30-14:00,Social event, Lunch,,
Invited talk,Sophie Donnet,Using a Sequential Monte Carlo algorithm to find the mesoscale structure of a network,"This work is motivated by the analysis of ecological interaction networks. Stochastic block models are widely used in this field to decipher the structure that underlies a network or that is shared by a collection of networks. Efficient algorithms based on variational approximations exist for frequentist inference and sometimes for Bayesian inference, but without statistical guaranties as for the resulting estimates. We propose to combine the variational estimation with a sequential Monte-Carlo algorithm to efficiently sample the posterior distribution and to perform model selection. "
Tuesday,15:00-15:30,Contributed,Francesca Crucinio,Optimal Scaling Results for a Wide Class of Proximal MALA Algorithms,"We consider a recently proposed class of MCMC methods which uses proximity maps instead of gradients to build proposal mechanisms which can be employed for both differentiable and non-differentiable targets. These methods have been shown to be stable for a wide class of targets, making them a valuable alternative to Metropolis-adjusted Langevin algorithms (MALA); and have found wide application in imaging contexts. The wider stability properties are obtained by building the Moreau-Yoshida envelope for the target of interest, which depends on a parameter λ. In this work, we investigate the optimal scaling problem for this class of algorithms, which encompasses MALA, and provide practical guidelines for the implementation of these methods."
Tuesday,15:30-16:00,Contributed,Filippo Ascolani,Complexity of Gibbs samplers through Bayesian asymptotics,"Gibbs samplers are popular algorithms to approximate posterior distributions arising from Bayesian hierarchical models.  Despite their popularity and good empirical performances, however, there are still relatively few quantitative theoretical results on their scalability or lack thereof, e.g. much less than for gradient-based sampling methods. We introduce a novel technique to analyse the asymptotic behaviour of mixing times of Gibbs Samplers, based on tools of Bayesian asymptotics.  We apply our methodology to high-dimensional hierarchical models, obtaining dimension-free convergence results for Gibbs samplers under random data-generating assumptions, for a broad class of two-level models with generic likelihood function. Specific examples with Gaussian, binomial and categorical likelihoods are discussed."
Tuesday,16:00-16:30,Social event, Coffee Break,,
Tuesday,16:30-17:30,Invited talk,Jean-Michel Marin,Goodness of Fit for Bayesian Generative Models,"Goodness-of-fit methods (GOF) aim at evaluating the level of adequacy between the observed dataset and a given model of interest, typically using an hypothesis-testing approach. In an Approximate Bayesian Computation context, this question can be re-framed as a novelty detection problem, in which one seeks to evaluate to which extent the observed dataset is an outlier compared to the simulated datasets. Many scores have been used as metrics to construct GOF test statistics and have been extensively tested in the literature. Here we propose a score based on the Local Outlier Factor. "
Tuesday,18:00-19:00,Social event,Football game,,
Tuesday,19:30-21:00,Social event, Dinner,,
Wednesday,9:00-11:00,Master Class,Silvia Chiappa,,
Wednesday,11:00-11:30,Social event, Coffee Break,,
Wednesday,11:30-12:00,Contributed,Jacopo Iollo,Tempered sequential Monte Carlo for Bayesian experimental design via stochastic optimization,"We propose a new procedure, for Bayesian experimental design, that performs sequential design optimization while simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and tempered Sequential Monte Carlo (SMC) samplers to maximise the  Expected Information Gain (EIG).  As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective worsens classical SMC performance.  To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling.  %tempering is proposed to improve SMC sampling accuracy.  This novel combination of stochastic optimization and  tempered SMC  allows to jointly handle design optimization and  parameter posterior inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the approach potential on various benchmarks where our procedure outperforms other recent existing approaches."
Wednesday,12:00-12:30,Contributed,Mengyan Zhang,Bayesian optimisation with aggregated feedback,"We consider the Bayesian optimisation problem, under a novel setting of aggregated feedback. This is motivated by applications where the precise rewards are impossible or expensive to obtain, while an aggregated reward or feedback, such as the average over a subset, is available. We adaptively construct a tree with nodes as subsets of the arm space and propose Gaussian Process Optimistic Optimisation (GPOO) algorithm."
Wednesday,12:30-14:00,Social event,Lunch,,
Wednesday,14:00-19:30,Social event,Free time,,
Wednesday,19:30-21:00,Social event,Dinner,,
Wednesday,21:00-22:30,Social event,Pub Quizz,,
Thursday,9:00-11:00,Master Class,Silvia Chiappa,,
Thursday,11:00-11:30,Social event, Coffee Break,,
Thursday,11:30-12:00,Contributed,Elena Bortolato,Coupling MCMC algorithms on submanifolds,"Manifolds arise in diverse problems in statistics, demanding effective methods for sampling probability distributions defined on them and specialized Markov Chain Monte Carlo (MCMC) methods since Andersen (1983) were developed.Such techniques can be also applied to a variety of sampling problems, even not  naturally defined on manifolds. We propose to combine standard MCMC samplers with such manifold steps and develop implementable coupling schemes to diagnose convergence and to evaluate the efficiency."
Thursday,12:00-12:30,Contributed,Thibaut Lemoine,Monte Carlo integration on complex manifolds,TBC
Thursday,12:30-14:00,Social event, Lunch,,
Thursday,14:00-16:00,Tutorial,Gabriel Victorino Cardoso and Yazid Janati El Idrissi,,
Thursday,16:00-16:30,Social event, Coffee Break,,
Thursday,16:30-17:30,Invited talk,Fabrizia Mealli,,
Thursday,19:00-21:30,Social event, Bouillabaisse, ,
Thursday,21:30-23:00,Social event,Karaoke night,,
Friday,9:00-10:00,,,
Friday,10:00-10:30,Contributed,Anna Menacher,Scalar-on-image regression with a relaxed Gaussian process prior,"In this work, we propose a Bayesian nonparametric scalar-on-image regression model, where the output is a scalar and the image is treated as the input, with a thresholded Gaussian process prior on the spatially-varying coefficients in order to introduce sparsity and smoothness into the model. Specifically, we apply a class of piecewise-smooth, sparse, and continuous functions, called the relaxed-thresholded Gaussian process (RTGP), that are flexible enough to exhibit both properties of hard- and soft-thresholding Gaussian processes to tackle the common non-identifiability issue of scalar-on-image regression problems. Our main contribution is the improved scalability, allowing for larger sample sizes and bigger image dimensions, which is made possible by replacing posterior sampling with a variational approximation. Moreover, the reduction in computational cost enables the vertex-wise analysis of cortical surface data or the voxel-wise analysis of volumetric data which previously was too prohibitive to perform for large-scale Bayesian spatial models with thresholded Gaussian process priors. Lastly, we validate our results via extensive simulation studies and an application to the Adolescent Brain Cognition Development (ABCD) study, a large-scale neuroimaging application with a sample size of over 5,000 subjects."
Friday,10:30-11:00,Contributed,Claudio del Sole,Hierarchically dependent mixture hazard rates for modelling competing risks,"A popular approach in Bayesian modelling of partially exchangeable data consists in imposing hierarchical nonparametric priors, which induce dependence across groups of observations. In survival analysis, hierarchies of completely random measures have been successfully exploited as mixing measures to model multivariate dependent mixture hazard rates, leading to a posterior characterization which may also accommodate censored observations. Such framework can be easily adapted to a competing risks scenario, in which groups correspond to different diseases affecting each individual: in this case, the multivariate construction acts at a latent level, as only the minimum time-to-event and the corresponding cause of death are actually observed. The posterior hierarchy of random measures, as well as the posterior estimates of both survival function and cause-specific incidence functions are explicitly described, conditionally on a suitable latent partition structure which fits the Chinese restaurant franchise metaphor. Marginal and conditional sampling algorithms are also devised and tested on synthetic datasets. The performances of this proposal are finally compared with those of its non-hierarchical counterpart, which models the hazard rate of each disease independently: leveraging the information borrowed from other groups, the hierarchical construction is empirically shown to recover the shape of the incidence functions more efficiently, in presence of proportional hazards."
Friday,11:00-11:30,Social event, Coffee Break,,
Friday,11:30-12:00,Contributed,Felipe Uribe,Towards dimension reduction of Bayesian inverse problems with neural network priors,"In many Bayesian inverse problems the change from prior to posterior is confined to a low-dimensional subspace of the parameter space. We explore gradient-based dimension reduction techniques to identify this crucial subspace, with the primary aim of enhancing the efficiency of MCMC methods employed in tackling the inverse problem."
Friday,12:00-12:30,Introduction,Organizers,Closing remarks,Bye everyone! Hope you enjoyed the conference!
Friday,12:30-14:00,Social event, Lunch,,
